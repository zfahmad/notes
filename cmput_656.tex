\documentclass[10pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[left=1.25in,right=1.25in]{geometry}
\usepackage{fancyhdr}

\renewcommand{\baselinestretch}{1.5}
\newcommand{\indmax}{\operatorname{ind\,max}}
\newcommand{\argmax}{\operatorname{arg\,max}}

\theoremstyle{definition}
\newtheorem{exmp}{Example}

\pagestyle{fancy}
\lhead{CMPUT 656}
\rhead{Notes}

\title{CMPUT 656 Notes}
\author{Z. F. Ahmad}
\date{}

\begin{document}

\section*{Bregman Divergences}

Given a strictly convex, differentiable function $f:\mathcal{X}\rightarrow \mathbb{R}$ in the compact space $\mathcal{X} \subseteq \mathbb{R}^n$, the \emph{Bregman divergence} between two points $\mathbf{x}_1$ and $\mathbf{x}_2$ in the space of $\mathcal{X}$ is defined to be

\begin{equation}
D_f(\mathbf{x}_1\lVert\mathbf{x}_2) = f(\mathbf{x}_1) - f(\mathbf{x}_2) - \langle\nabla f(\mathbf{x}_2), \mathbf{x}_1 - \mathbf{x}_2\rangle, \quad{} \forall \mathbf{x}_1,\mathbf{x}_2 \in \mathcal{X}
\end{equation}

\noindent
That is, the Bregman divergence of a function $f$ between two points $\mathbf{x}_1$ and $\mathbf{x}_2$ is the difference of the value of $f$ at $\mathbf{x}_1$ and the value of the first-order Taylor expansion of $f$ around $\mathbf{x}_2$ evaluated at $\mathbf{x}_1$.

\begin{exmp}

\noindent
For the Euclidean distance function, 
$$
f(\mathbf{x}) = \frac{1}{2} \lVert \mathbf{x} \rVert ^2
$$
$$
\nabla f(\mathbf{x}) = \mathbf{x}
$$

The Bregman divergence between any two points in the space of this function can be derived as follows:
\begin{equation}
\begin{aligned}
D_f(\mathbf{x}\lVert\mathbf{y}) & = \frac{1}{2} \lVert \mathbf{x} \rVert ^2 - \frac{1}{2} \lVert \mathbf{y} \rVert ^2 - \langle \mathbf{y}, \mathbf{x} - \mathbf{y} \rangle \\
& = \frac{1}{2}\left( \sqrt{\sum_i x_i^2} \right)^2 - \frac{1}{2}\left( \sqrt{\sum_i y_i^2} \right)^2
- \sum_i x_i y_i + \sum_i y_i^2 \\
& = \frac{1}{2}\sum_i x_i^2 - \frac{1}{2}\sum_i y_i^2 - \sum_i x_i y_i + \sum_i y_i^2 \\
& = \frac{1}{2}\sum_i x_i^2 - \sum_i x_i y_i + \frac{1}{2}\sum_i y_i^2 \\
& = \frac{1}{2}\sum_i (x_i^2 - 2 x_i y_i + y_i^2) \\
& = \frac{1}{2}\sum_i (x_i - y_i)^2 \\
\therefore D_f(\mathbf{x}\lVert\mathbf{y}) & = \frac{1}{2} \lVert \mathbf{x} - \mathbf{y} \rVert^2
\end{aligned}
\end{equation}

\end{exmp}

\subsection*{Conjugate Functions}

Given a function $f:\mathcal{X} \rightarrow \mathbb{R}$ where $\mathcal{X} \subseteq \mathbb{R}^n$, the convex conjugate function $f^*$ is defined as
\begin{equation}
f^*(\mathbf{y}) = \sup_{\mathbf{x}} \left[\mathbf{y}^\top \mathbf{x} - f(\mathbf{x})\right]
\end{equation}
\noindent
If the function $f$ is differentiable and convex, then $\sup_{\mathbf{x}}\left[\mathbf{y}^\top \mathbf{x} - f(\mathbf{x})\right]$ can be easily calculated by taking the derivative w.r.t. $\mathbf{x}$ to get

\begin{equation}
\begin{aligned}
\frac{d}{d\mathbf{x}} \left(\mathbf{y}^\top \mathbf{x} - f(\mathbf{x})\right) & = 0 \\
\Rightarrow \mathbf{y} & = \nabla f(\mathbf{x})
\end{aligned}
\end{equation}

\subsection*{Conjugate Duality Relationship}

Given a function $f:\mathcal{X} \rightarrow \mathbb{R}$ where $\mathcal{X} \subseteq \mathbb{R}^n$ and its convex conjugate $f^* = \sup_{\mathbf{x}} \left[\mathbf{y}^\top \mathbf{x} - f(\mathbf{x})\right]$, we can define the following

$$
\mathbf{y}_1 = \nabla f(\mathbf{x}_1)
$$
$$
\mathbf{y}_2 = \nabla f(\mathbf{x}_2)
$$
$$
\mathbf{x}_1 = \nabla f^*(\mathbf{y}_1)
$$
$$
\mathbf{x}_2 = \nabla f^*(\mathbf{y}_2)
$$

\noindent
We can then show that measuring the divergence in both spaces are equivalent:

\begin{equation}
\begin{aligned}
D_f(\mathbf{x}_1\lVert\mathbf{x}_2) & = f(\mathbf{x}_1) - f(\mathbf{x}_2) - \langle\nabla f(\mathbf{x}_2), \mathbf{x}_1 - \mathbf{x}_2\rangle \\
& = f(\mathbf{x}_1) - f(\mathbf{x}_2) - (\mathbf{x}_1 - \mathbf{x}_2)^\top\mathbf{y}_2 \\
& = f(\mathbf{x}_1) - f(\mathbf{x}_2) - (\mathbf{x}_1 - \mathbf{x}_2)^\top\mathbf{y}_2 + \mathbf{x}_1^\top\mathbf{y}_1 - \mathbf{x}_1^\top\mathbf{y}_1 \\
& = f(\mathbf{x}_1) - f(\mathbf{x}_2) - \mathbf{x}_1^\top\mathbf{y}_2 + \mathbf{x}_2^\top\mathbf{y}_2 + \mathbf{x}_1^\top\mathbf{y}_1 - \mathbf{x}_1^\top\mathbf{y}_1 \\
& = -[\mathbf{x}_1^\top\mathbf{y}_1 - f(\mathbf{x}_1)] + [\mathbf{x}_2^\top\mathbf{y}_2 - f(\mathbf{x}_2)] - \mathbf{x}_1^\top\mathbf{y}_2 + \mathbf{x}_1^\top\mathbf{y}_1 \\
& = -f^*(\mathbf{y}_1) + f^*(\mathbf{y}_2) - (\mathbf{y}_2 - \mathbf{y}_1)^\top\mathbf{x}_1 \\
& = f^*(\mathbf{y}_2) - f^*(\mathbf{y}_1) - \langle \nabla f^*(\mathbf{y}_1), \mathbf{y}_2 - \mathbf{y}_1 \rangle \\
& = D_{f^*}(\mathbf{y}_2\lVert\mathbf{y}_1)
\end{aligned}
\end{equation}

\section*{KL-Divergence}
$$
f_\tau(\mathbf{q}) = \ln{\mathbf{1}^\top e^{\mathbf{q}/\tau}}
$$
$$
\nabla f_\tau(\mathbf{q}) = \frac{e^{\mathbf{q}/\tau}}{\mathbf{1}^\top e^{\mathbf{q}/\tau}}
$$
$$
\mathcal{F} = \{\mathbf{q}: \mathbf{q}_0=0\}
$$
$$
f^*_\tau(\mathbf{p}) = \tau \mathbf{p}^\top\ln{\mathbf{p}}
$$
$$
\nabla f^*_\tau(\mathbf{p}) = \tau (\ln{\mathbf{p}} - \ln{\mathbf{p}_0})
$$
$$
\mathcal{F}^* = \{\mathbf{p}:\mathbf{p}\geq0, \mathbf{1}^\top \mathbf{p} = 1\}
$$

\section*{Jacobian of $\mathbf{p}$ w.r.t. $\mathbf{q}$}
\noindent
Suppose we are given the softmax function $\nabla f$ where
$$
\mathbf{p} = \nabla f(\mathbf{q}) = \frac{e^\mathbf{q}}{\sum_i e^{\mathbf{q}_i}}
$$

\noindent
Taking partial derivates with respect to the components of $\mathbf{q}$ gives us

$$
\frac{\partial p_i}{\partial q_j} = \frac{\partial}{\partial q_j} \left(\frac{e^{q_i}}{\sum_j e^{q_j}} \right) = \frac{\partial}{\partial q_j} \left(\frac{-e^{q_i}e^{q_j}}{\sum_j e^{q_j}} \right), \quad \text{when $i\neq j$}
$$

\noindent
and

$$
\frac{\partial p_i}{\partial q_j} = \frac{\partial}{\partial q_j} \left(\frac{e^{q_i}}{\sum_j e^{q_j}} \right) = \frac{\partial}{\partial q_j} \left(\frac{e^{q_i}(\sum_j e^{q_j})-e^{q_i}e^{q_j}}{\sum_j e^{q_j}} \right), \quad \text{when $i=j$}
$$

\noindent
Combining both equations, we get the matrix equation

\begin{equation*}
\begin{aligned}
\frac{d\mathbf{p}}{d\mathbf{q}} & = \triangle(\nabla f(\mathbf{q})) - \nabla f(\mathbf{q})\nabla f(\mathbf{q})^\top, \\
& = \triangle(\mathbf{p}) - \mathbf{p}\mathbf{p}^\top
\end{aligned}
\end{equation*}

\section*{Policy Gradient Method}

In reinforcement learning, one method for find the optimal action (or optimal sequence of actions) is to use \emph{policy gradient methods}. These methods approximate a policy using the rewards and value function approximation. The objective of policy gradient methods is to find the policy with the best performance

$$
\max_{\pi} \pi \cdot r
$$

\noindent
where $\pi$ is a policy. This objective can be rewritten as
$$
V = \mathbf{p} \cdot \mathbf{r}
$$
$$
\mathbf{p} = \nabla f(\mathbf{q})
$$

\noindent
where $\mathbf{q}$ is a vector of values produced from some value function approximator (i.e., a neural network) parameterized by a weight vector $\pmb{\theta}$ so that $\nabla f(\mathbf{q}) = \nabla f(\mathbf{q}(\pmb{\theta}))$. Finding the maximum policy would require taking the derivative of $V$ with respect to the parameters for $\mathbf{q}$ (which also parameterize $\mathbf{p}$) to get

\begin{equation*}
\begin{aligned}
\frac{dV}{d\pmb{\theta}} & = \frac{d\mathbf{q}}{d\pmb{\theta}} \cdot \frac{d\mathbf{p}}{d\mathbf{q}} \cdot \frac{dV}{d\mathbf{p}} \\
& = \frac{d\mathbf{q}}{d\pmb{\theta}} \cdot (\triangle(\mathbf{p}) - \mathbf{p}\mathbf{p}^\top) \cdot \mathbf{r}
\end{aligned}
\end{equation*}

\noindent
Depending on the direction in which we pull out $\triangle(\mathbf{p}$ from the middle term, we can derive two resultant forms. The first form, called ``Bregman'', being

\begin{equation*}
\begin{aligned}
\frac{dV}{d\pmb{\theta}} & = \frac{d\mathbf{q}}{d\pmb{\theta}} \cdot \triangle(\mathbf{p}) (I - \mathbf{1}\mathbf{p}^\top) \cdot \mathbf{r} \\
& = \sum_a p_a \left[\frac{d\mathbf{q}}{d\pmb{\theta}}\right]_{:a}(r_a - \mathbf{p}^\top \mathbf{r})
\end{aligned}
\end{equation*}

\noindent 
The second form is called ``Williams'' and is of the form

\begin{equation*}
\begin{aligned}
\frac{dV}{d\pmb{\theta}} & = \frac{d\mathbf{q}}{d\pmb{\theta}} \cdot (I - \mathbf{1}\mathbf{p}^\top) \triangle(\mathbf{p}) \cdot \mathbf{r} \\
& = \sum_a p_a \left[\frac{d\mathbf{q}}{d\pmb{\theta}}(I - \mathbf{p}\mathbf{1}^\top)\right]_{:a}r_a
\end{aligned}
\end{equation*}

\noindent
With the KL-divergence we know that

\begin{equation*}
\begin{aligned}
\nabla f(\mathbf{q}) & = \frac{e^{\mathbf{q}}}{\mathbf{1}^\top e^{\mathbf{q}}} \\
\mathbf{p} & = \frac{e^{\mathbf{q}}}{\mathbf{1}^\top e^{\mathbf{q}}} \\
\Rightarrow \ln\mathbf{p} & = \ln\left(\frac{e^{\mathbf{q}}}{\mathbf{1}^\top e^{\mathbf{q}}}\right) \\
& = \ln e^{\mathbf{q}} - \ln\mathbf{1}^\top e^{\mathbf{q}} \\
& = \mathbf{q} - \mathbf{1}f(\mathbf{q}) \\
\therefore \frac{d\log\mathbf{p}}{d\mathbf{q}} & = \mathbf{I} - \nabla f(\mathbf{q})\mathbf{1}^\top = \mathbf{I} - \mathbf{p}\mathbf{1}^\top 
\end{aligned}
\end{equation*}

\noindent
As a result, Williams can be rewritten as

$$
\sum_a p_a \frac{d\mathbf{q}}{d\pmb{\theta}}\left[\frac{d\log\mathbf{p}}{d\mathbf{q}}\right]_{:a}(r_a - c)
$$

\section*{Bregman Losses for Bandits}

Suppose we have a vector of actions $\mathbf{a} = \langle a_1, a_2, \hdots, a_n\rangle$, where each action $a_i$ is associated with a real-valued reward $r_i = q_i$ and $\mathbf{q}$ is the vector of rewards $\langle q_1, q_2, \hdots, q_n\rangle$. Our objective is to estimate a selection policy $\mathbf{\hat{p}}$ that selects the optimal action (the action that achieves the highest expected reward.) In our case, the policy over actions is a function of action values given as

$$
\mathbf{\hat{p}} = \nabla f(\mathbf{\hat{q}})
$$

\noindent
where $\mathbf{\hat{q}}$ is a vector of values produced from some value function approximator (i.e., a neural network) parameterized by a weight vector $\pmb{\theta}$ so that $\nabla f(\mathbf{\hat{q}}) = \nabla f(\mathbf{\hat{q}(\pmb{\theta})})$. Selecting actions by $\epsilon$-greedy, we can say that

$$
\mathbf{\hat{p}} = (1 - \lambda\epsilon)\cdot\indmax (\mathbf{\hat{q}}) + \epsilon\mathbf{1}
$$

\noindent
Taking an action $a$ we observe a reward $q_a$. What are the possible loss functions that we can use?

\begin{equation*}
\begin{aligned}
L(\hat{q}_a,q_a) & = \sum_a\frac{1}{\lvert A \rvert}(\hat{q}_a - q_a) \quad \text{[Expected Square Loss]} \\
& = \sum_a\hat{p}_a(\hat{q}_a - q_a) \quad \text{[]} \\
& = \sum_a p_a(\hat{q}_a - q_a) \quad \text{[Importance Sampling Weighted Expected Square Loss]} \\
& = \sum_a \hat{p}_a q_a + \nabla f^*(\hat{p}_a) \quad \text{[Policy Gradient with Regularized Entropy]}
\end{aligned}
\end{equation*}

\section*{Feedforward Neural Networks}

\subsection*{Calculus Chain Rule}

Figure~\ref{fig:sl_nn} depicts a single-layer neural network that takes an input vector $\mathbf{x}$ where $\lvert \mathbf{x} \rvert = n$ and produces a scalar output $f(\mathbf{x})$. The layer $\mathbf{h}$ produces a set of output values that is given by $h(\mathbf{x})$. The network output is calculated by $g(h(\mathbf{x}))$. The gradient of the function can be calculated using the chain rule as follows:

\begin{figure}[t]
\centering
\includegraphics[scale=1]{nn_chain}
\caption{Single-layer neural network}
\label{fig:sl_nn}
\end{figure}

\begin{equation*}
\begin{aligned}
f(\mathbf{x}) & = g(h(\mathbf{x})) \\
\frac{df}{dx_j} & = \sum\limits_i \frac{dg}{dh_i} \cdot \frac{dh_i}{dx_j}, \quad \forall j = 0, 1, \hdots n\\
\therefore \left[\frac{df}{d\mathbf{x}}\right]_{n\times 1} & = \left[\frac{d\mathbf{h}}{d\mathbf{x}}\right]_{n\times m} \cdot \left[\frac{dg}{d\mathbf{h}}\right]_{m\times 1}
\end{aligned}
\end{equation*}

\subsection*{Backpropagation}

Let us assume we have a neural network with $L$ hidden layers. The input vector $\mathbf{x}$ is transformed by a weight matrix $\mathbf{W}^{(1)}$ to produce a layer represented as the vector $\mathbf{z}^{(1)}$. In turn, $\mathbf{z}^{(1)}$ is transformed by a function $\phi(\mathbf{z}^{(1)})$ to produce a vector $\mathbf{y}^{(1)}$. Notating the input vector as $\mathbf{x} = \mathbf{y}^{(0)}$, we can generalize over numerous layers to get a sequence of transformations such that

$$
\mathbf{y}^{(0)} \xrightarrow{\mathbf{W}^{(1)}} \mathbf{z}^{(1)} \xrightarrow{\phi(\mathbf{z}^{(1)})} \mathbf{y}^{(1)} \xrightarrow{\mathbf{W}^{(2)}} \mathbf{z}^{(2)} \rightarrow \hdots \rightarrow \mathbf{z}^{(L)} \xrightarrow{\phi(\mathbf{z}^{(L)})} \mathbf{y}^{(L)}
$$

\noindent
where at any layer $l$ we have

$$
\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \cdot \mathbf{y}^{(l-1)}
$$
$$
\mathbf{y}^{(l)} = \phi(\mathbf{z}^{(l)}).
$$

\noindent
It is important to note that each layer is appended with a bias term.

The neural network is trained on a number of the samples and the output of the network is evaluated against the true output value $\mathbf{y}^*$ of the training data using a Bregman loss function\footnote{Even though the output is in $\mathbf{y}$, the problem is a convex training in $\mathbf{z}$.}:

\begin{equation*}
\begin{aligned}
\text{Loss}(\mathbf{y}^*, \mathbf{y}^{(L)}) = \mathcal{L} & \overset{\Delta}{=} D_{f^*}(\mathbf{y}^*, \mathbf{y}^{(L)}) \\
& = D_{f}(\mathbf{z}^{(L)}, \nabla f^*(\mathbf{y}^*)) \\
& = f(\mathbf{z}^{(L)}) - \langle \mathbf{z}^{(L)},\mathbf{y}^* \rangle + f^*(\mathbf{y}^*)
\end{aligned}
\end{equation*}

\noindent
The gradient of the loss function with respect to $\mathbf{z}^{(L)}$ is given by

\begin{equation*}
\begin{aligned}
\frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} & = \frac{d}{d\mathbf{z}^{(L)}}\left(f(\mathbf{z}^{(L)}) - \langle \mathbf{z}^{(L)},\mathbf{y}^* \rangle + f^*(\mathbf{y}^*) \right) \\
& = \nabla f(\mathbf{z}^{(L)}) - \mathbf{y}^* \\
& = \mathbf{y}^{(L)} - \mathbf{y}^* = \pmb{\delta}^{(L)}
\end{aligned}
\end{equation*}

\noindent
and the gradient of the loss function with respect to the final weight matrix is

\begin{equation*}
\begin{aligned}
\frac{d\mathcal{L}}{d\mathbf{W}^{(L)}_{k:}} & = \frac{d\mathcal{L}}{dz^{(L)}_{k}} \cdot \frac{dz^{(L)}_{k}}{d\mathbf{W}^{(L)}_{k:}} \\
& = \delta^{(L)}_{k}\mathbf{y}^{(L-1)\top} \\
\therefore \frac{d\mathcal{L}}{d\mathbf{W}^{(L)}} & = \pmb{\delta}^{(L)}\mathbf{y}^{(L-1)\top}
\end{aligned}
\end{equation*}

\noindent
and finally the gradient of the loss with respect to the output vector $\mathbf{y}^{(L-1)}$ is

\begin{equation*}
\begin{aligned}
\frac{d\mathcal{L}}{d\mathbf{y}^{(L-1)}} & = \sum\limits_k \frac{d\mathcal{L}}{dz^{(L)}_{k}} \cdot \frac{dz^{(L)}_{k}}{d\mathbf{y}^{(L-1)}} \\
& = \sum\limits_k \delta_k^{(L)} \mathbf{W}^{(L)\top}_{k:} \\
\therefore \frac{d\mathcal{L}}{d\mathbf{y}^{(L-1)}} & = \mathbf{W}^{(L)\top} \pmb{\delta}^{(L)}.
\end{aligned}
\end{equation*}

\noindent
Continuing on to the penultimate layer of the neural network $L-1$, we can derive the gradient of the loss with respect to the structures in the that layer. The gradient of the loss with respect to $\mathbf{z}^{(L-1)}$ is found by

\begin{equation*}
\begin{aligned}
\frac{d\mathcal{L}}{dz_k^{(L-1)}} & = \frac{d\mathcal{L}}{dy^{(L-1)}} \cdot \phi'(z_k^{(L-1)}) \\
\frac{d\mathcal{L}}{d\mathbf{z}^{(L-1)}} & = \frac{d\mathcal{L}}{d\mathbf{y}^{(L-1)}} \cdot \phi'(\mathbf{z}^{(L-1)}) = \pmb{\delta}^{(L-1)}
\end{aligned}
\end{equation*}

\noindent
and subsequently

\begin{equation*}
\begin{aligned}
\frac{d\mathcal{L}}{d\mathbf{W}^{(L-1)}_{k:}} & = \frac{d\mathcal{L}}{dz^{(L-1)}_{k}} \cdot \frac{dz^{(L-1)}_{k}}{d\mathbf{W}^{(L-1)}_{k:}} \\
& = \delta^{(L-1)}_{k}\mathbf{y}^{(L-2)\top} \\
\therefore \frac{d\mathcal{L}}{d\mathbf{W}^{(L-1)}} & = \pmb{\delta}^{(L-1)}\mathbf{y}^{(L-2)\top}
\end{aligned}
\end{equation*}

\begin{equation*}
\frac{d\mathcal{L}}{d\mathbf{y}^{(L-2)}} = \mathbf{W}^{(L-1)\top} \pmb{\delta}^{(L-1)}.
\end{equation*}

\noindent
So we can write, at any layer $l$, assuming we have $\frac{d\mathcal{L}}{d\mathbf{y}^{(l)}}$

\begin{equation*}
\frac{d\mathcal{L}}{d\mathbf{z}^{(l)}} = \frac{d\mathcal{L}}{d\mathbf{y}^{(l)}} \cdot \phi'(\mathbf{z}^{(l)}) \overset{\Delta}{=} \pmb{\delta}^{(l)}
\end{equation*}

\begin{equation*}
\frac{d\mathcal{L}}{d\mathbf{W}^{(l)}} = \pmb{\delta}^{(l)}\mathbf{y}^{(l-1)\top}
\end{equation*}

\begin{equation*}
\frac{d\mathcal{L}}{d\mathbf{z}^{(l)}} = \mathbf{W}^{(l)\top} \pmb{\delta}^{(l)}
\end{equation*}

\subsection*{Transfer Functions, $\phi$}

$$
\phi(\mathbf{z}) = \sigma(\mathbf{z}) = \frac{1}{1 + e^{\mathbf{z}}}, \quad [\text{Sigmoid}]
$$
$$
\phi(\mathbf{z}) = \tanh(\mathbf{z}) = \frac{e^{\mathbf{z}} - e^{-\mathbf{z}}}{e^{\mathbf{z}} + e^{-\mathbf{z}}}, \quad [\text{tanh}]
$$
$$
\phi(\mathbf{z}) = \lvert\mathbf{z}\rvert, \quad [\text{Absolute}]
$$
$$
\phi(\mathbf{z}) = \mathbf{z}_{+} = \max(0,\mathbf{z}), \quad [\text{Relu}]
$$
$$
\phi(\mathbf{z}) = f_{\tau} = \tau\log(e^{\mathbf{z}/\tau} + 1), \quad [\text{KL-Divergence}]
$$

\subsubsection*{Relu Gate}

It is positive homogeneous and so

$$
\phi(a\mathbf{z}) = a\phi(\mathbf{z}), \quad \forall a \geq 0
$$
$$
\phi(\mathbf{z}) = \mathbf{z}\cdot\phi'(\mathbf{z})
$$
$$
\phi(\mathbf{z}) = \mathbf{z}\cdot\mathbb{I}(\mathbf{z}\geq0)
$$

\noindent
The gradients for the NN can then be calculated as

\begin{equation*}
\begin{aligned}
\frac{d\mathcal{L}}{d\mathbf{z}^{(l)}} & = \frac{d\mathcal{L}}{d\mathbf{y}^{(l)}}\circ\phi'(\mathbf{z}^{(l)}) \\
& = \triangle(\mathbb{I}(\mathbf{z}\geq0))\cdot\frac{d\mathcal{L}}{d\mathbf{y}^{(l)}}
\end{aligned}
\end{equation*}

\noindent
where $\circ$ is the component-wise product (Hadamard product) and

\begin{equation*}
\frac{d\mathcal{L}}{d\mathbf{W}^{(l)}} = \pmb{\delta}^{(L\rightarrow l)}\mathbf{y}^{(l-1)\top}
\end{equation*}

\begin{equation*}
\pmb{\delta}^{(L\rightarrow l)} = \triangle(\mathbf{y}^{(l)})\mathbf{W}^{(l+1)\top}\triangle(\mathbf{y}^{(l+1)})\mathbf{W}^{(l+2)\top}\hdots\mathbf{W}^{(L-1)\top}\triangle(\mathbf{y}^{(L-1)})\pmb{\delta}^{(L)}
\end{equation*}

\section*{One-shot Bandits}

Given

\begin{itemize}
\item reward vector $\langle r_i, \hdots, r_n \rangle$
\item policy vector $\langle \hat{p}, \hdots, \hat{p} \rangle$
\item value vector $\langle \hat{q}, \hdots, \hat{q} \rangle$
\end{itemize}

The greedy policy $\rightarrow p^* = \indmax(r)$

Optimal regularized policy 
\begin{equation*}
\begin{aligned}
\mathbf{p} & = \nabla f_{\tau}(\textbf{q}) = \nabla f_{\tau}(\mathbf{r}) \\
& = \argmax\limits_{\mathbf{p} \in \mathcal{F}^*} \mathbf{r}\cdot \mathbf{p} - F^*_{\tau}(\mathbf{p}) \\
\therefore V_{\text{max}} & = \mathbf{p}\cdot \textbf{r} - F^*_{\tau}(\mathbf{p}) = F_{\tau}(\mathbf{r})
\end{aligned}
\end{equation*}

\subsubsection*{Loop}
\begin{enumerate}
\item Select $a_i \sim \mathbf{\hat{p}}$
\item Sample $r_{a_i}$
\item Update $\mathbf{\hat{q}}$
\item Update $\mathbf{\hat{p}}$
\end{enumerate}

\end{document}